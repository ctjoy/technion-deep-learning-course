{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bm}[1]{{\\bf #1}}\n",
    "\\newcommand{\\bb}[1]{\\bm{\\mathrm{#1}}}\n",
    "$$\n",
    "\n",
    "# Part 2: Variational Autoencoder\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will learn to generate new data using a special type of autoencoder model which allows us to \n",
    "sample from it's latent space. We'll implement and train a VAE and use it to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the dataset\n",
    "<a id=part2_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by downloading a dataset of images that we want to learn to generate. \n",
    "We'll use the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) (LFW) dataset which contains many labels faces of famous individuals.\n",
    "\n",
    "We're going to train our generative model to generate a specific face, not just any face.\n",
    "Since the person with the most images in this dataset is former president George W. Bush, we'll set out to train a Bush Generator :)\n",
    "\n",
    "However, if you feel adventurous and/or prefer to generate something else, feel free to edit the `PART2_CUSTOM_DATA_URL` variable in `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs236605.plot as plot\n",
    "import cs236605.download\n",
    "from hw3.answers import PART2_CUSTOM_DATA_URL as CUSTOM_DATA_URL\n",
    "\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "if CUSTOM_DATA_URL is None:\n",
    "    DATA_URL = 'http://vis-www.cs.umass.edu/lfw/lfw-bush.zip'\n",
    "else:\n",
    "    DATA_URL = CUSTOM_DATA_URL\n",
    "\n",
    "_, dataset_dir = cs236605.download.download_data(out_path=DATA_DIR, url=DATA_URL, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Dataset` object that will load the extraced images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "im_size = 64\n",
    "tf = T.Compose([\n",
    "    # Resize to constant spatial dimensions\n",
    "    T.Resize((im_size, im_size)),\n",
    "    # PIL.Image -> torch.Tensor\n",
    "    T.ToTensor(),\n",
    "    # Dynamic range [0,1] -> [-1, 1]\n",
    "    T.Normalize(mean=(.5,.5,.5), std=(.5,.5,.5)),\n",
    "])\n",
    "\n",
    "ds_gwb = ImageFolder(os.path.dirname(dataset_dir), tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see what we got. You can run the following block multiple times to display a random subset of images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot.dataset_first_n(ds_gwb, 50, figsize=(10,5), nrows=5)\n",
    "print(f'Found {len(ds_gwb)} images in dataset folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = ds_gwb[0]\n",
    "x0 = x0.unsqueeze(0).to(device)\n",
    "print(x0.shape)\n",
    "\n",
    "test.assertSequenceEqual(x0.shape, (1, 3, im_size, im_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variational Autoencoder\n",
    "<a id=part2_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a model which learns a representation of data in an **unsupervised** fashion (i.e without any labels).\n",
    "Recall it's general form from the lecture:\n",
    "\n",
    "<img src=\"imgs/autoencoder.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder maps an instance $\\bb{x}$ to a **latent-space** representation $\\bb{z}$.\n",
    "It has an encoder part, $\\Phi_{\\bb{\\alpha}}(\\bb{x})$ (a neural net with parameters $\\bb{\\alpha}$)\n",
    "and a decoder part, $\\Psi_{\\bb{\\beta}}(\\bb{z})$ (a neural net with parameters $\\bb{\\beta}$).\n",
    "\n",
    "While autoencoders can learn useful representations,\n",
    "generally it's hard to use them as generative models because there's no distribution we can sample from in the latent space. In other words, we have no way to choose a point $\\bb{z}$ in the latent space\n",
    "such that $\\Psi(\\bb{z})$ will end up on the data manifold in the instance space.\n",
    "\n",
    "<img src=\"imgs/ae_sampling.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE), first proposed by [Kingma and Welling](https://arxiv.org/pdf/1312.6114.pdf), addresses this issue by taking a probabilistic perspective. \n",
    "Briefly, a VAE model can be described as follows.\n",
    "\n",
    "We define, in Baysean terminology,\n",
    "- The **prior** distribution $p(\\bb{Z})$ on points in the latent space.\n",
    "- The **likelihood** distribution of a sample $\\bb{X}$ given a latent-space representation: $p(\\bb{X}|\\bb{Z})$.\n",
    "- The **posterior** distribution of points in the latent spaces given a specific instance: $p(\\bb{Z}|\\bb{X})$.\n",
    "- The **evidence** distribution $p(\\bb{X})$ which is the distribution of the instance space due to the generative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our variational **decoder** we'll further specify:\n",
    "\n",
    "- A parametric likelihood distribution, $p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) = \\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$. The interpretation is that given a latent $\\bb{z}$, we map it to a point normally distributed around the point calculated by our decoder neural network. Note that here $\\sigma^2$ is a hyperparameter while $\\vec{\\beta}$ represents the network parameters.\n",
    "- A fixed latent-space prior distribution of $p(\\bb{Z}) = \\mathcal{N}(\\bb{0},\\bb{I})$.\n",
    "\n",
    "This setting allows us to generate a new instance $\\bb{x}$ by sampling $\\bb{z}$ from the multivariate normal\n",
    "distribution, obtaining the instance-space mean $\\Psi _{\\bb{\\beta}}(\\bb{z})$ using our decoder network,\n",
    "and then sampling $\\bb{x}$ from $\\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variational **encoder** will approximate the posterior with a parametric distribution \n",
    "$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x}) \\sim\n",
    "\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$.\n",
    "The interpretation is that our encoder neural network, $\\Phi_{\\vec{\\alpha}}(\\bb{x})$, calculates\n",
    "the mean and variance of the posterior distribution, and samples $\\bb{z}$ based on them.\n",
    "An important nuance here is that our network can't contain any stochastic elements that\n",
    "depend on the model parameters, otherwise we won't be able to back-propagate to those parameters.\n",
    "So sampling $\\bb{z}$ from $\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$ is not an option.\n",
    "The solution is to use what's known as the **reparametrization trick**: sample from an isotropic Gaussian, \n",
    "i.e. $\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$ (which doesn't depend on trainable parameters), and calculate the latent representation as\n",
    "$\\bb{z} = \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) + \\bb{u}\\odot\\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a VAE model, we would like to maximize the evidence, $p(\\bb{X})$, because\n",
    "$\n",
    "p(\\bb{X}) = \\int p(\\bb{X}|{\\bb{z}})p(\\bb{z})d\\bb{z}\n",
    "$\n",
    "thus maximizing the likelihood of generated instances from over the entire latent space.\n",
    "\n",
    "The **VAE loss** can therefore be stated as minimizing $\\mathcal{L} = -\\mathbb{E}_{\\bb{x}} \\log p(\\bb{X})$.\n",
    "As we saw in the lecture, this expectation is intractable,\n",
    "but we can obtain a lower-bound for $p(\\bb{X})$ (the evidence lower bound, \"ELBO\"):\n",
    "\n",
    "$$\n",
    "\\log p(\\bb{X}) \\ge \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( \\log  p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) )\n",
    "-  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "$\n",
    "\\mathcal{D} _{\\mathrm{KL}}(q\\left\\|\\right.p) =\n",
    "\\mathbb{E}_{\\bb{z}\\sim q}\\left[ \\log \\frac{q(\\bb{Z})}{p(\\bb{Z})} \\right]\n",
    "$\n",
    "is the Kullback-Liebler divergence, which can be interpreted as the information gained by using the posterior $q(\\bb{Z|X})$ instead of the prior distribution $p(\\bb{Z})$.\n",
    "\n",
    "Using the ELBO, the VAE loss becomes,\n",
    "$$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \n",
    "\\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ -\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) \\right]\n",
    "+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "By remembering that the likelihood is a Gaussian distribution with a diagonal covariance and by applying the reparametrization trick, we can write the above as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \n",
    "\\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\n",
    "\\left[ \n",
    "\\frac{1}{2\\sigma^2}\\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2\n",
    "\\right]\n",
    "+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "<a id=part2_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously our model will have two parts, an encoder and a decoder.\n",
    "Since we're working with images, we'll implement both as deep **convolutional** networks, where the decoder is a \"mirror image\" of the encoder implemented with adjoint (AKA transposed) convolutions.\n",
    "Between the encoder CNN and the decoder CNN we'll implement the sampling from\n",
    "the parametric posterior approximator $q_{\\bb{\\alpha}}(\\bb{Z}|\\bb{x})$\n",
    "to make it a VAE model and not just a regular autoencoder (of course, this is not yet enough to create a VAE,\n",
    "since we also need a special loss function which we'll get to later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's implement just the CNN part of the Encoder network\n",
    "(this is not the full $\\Phi_{\\vec{\\alpha}}(\\bb{x})$ yet).\n",
    "As usual, it should take an input image and map to a activation volume of a specified depth.\n",
    "We'll consider this volume as the features we extract from the input image.\n",
    "Later we'll use these to create the latent space representation of the input.\n",
    "which will be our latent space representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `EncoderCNN` class in the `hw3/autoencoder.py` module.\n",
    "Implement any CNN architecture you like. If you need \"architecture inspiration\" you can see e.g. [this](https://arxiv.org/pdf/1512.09300.pdf) or [this](https://arxiv.org/pdf/1511.06434.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw3.autoencoder as autoencoder\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 1024\n",
    "encoder_cnn = autoencoder.EncoderCNN(in_channels, out_channels).to(device)\n",
    "print(encoder_cnn)\n",
    "\n",
    "h = encoder_cnn(x0)\n",
    "print(h.shape)\n",
    "\n",
    "test.assertEqual(h.dim(), 4)\n",
    "test.assertSequenceEqual(h.shape[0:2], (1, out_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the CNN part of the Decoder.\n",
    "Again this is not yet the full $\\Psi _{\\bb{\\beta}}(\\bb{z})$. It should take an activation volume produced\n",
    "by your `EncoderCNN` and output an image of the same dimensions as the Encoder's input was.\n",
    "This should be a CNN which is a \"mirror image\" of the the Encoder. For example, replace convolutions with transposed convolutions, downsampling with up-sampling etc.\n",
    "Consult the documentation of [ConvTranspose2D](https://pytorch.org/docs/0.4.1/nn.html#convtranspose2d)\n",
    "to figure out how to reverse your convolutional layers in terms of input and output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `DecoderCNN` class in the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cnn = autoencoder.DecoderCNN(in_channels=out_channels, out_channels=in_channels).to(device)\n",
    "print(decoder_cnn)\n",
    "x0r = decoder_cnn(h)\n",
    "print(x0r.shape)\n",
    "\n",
    "test.assertEqual(x0.shape, x0r.shape)\n",
    "\n",
    "# Should look like colored noise\n",
    "T.functional.to_pil_image(x0r[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the full VAE Encoder, $\\Phi_{\\vec{\\alpha}}(\\vec{x})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\vec{h}$ from the input image $\\vec{x}$.\n",
    "2. Use two affine transforms to convert the features into the mean and log-variance of the posterior, i.e.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "        \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\mu}} + \\vec{b}_{\\mathrm{h\\mu}} \\\\\n",
    "        \\log\\left(\\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x})\\right) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\sigma^2}} + \\vec{b}_{\\mathrm{h\\sigma^2}}\n",
    "    \\end{align}\n",
    "    $$\n",
    "3. Use the **reparametrization trick** to create the latent representation $\\vec{z}$.\n",
    "\n",
    "Note that we model the **log** of the variance, not the actual variance.\n",
    "The reason is that the log is easier to optimize, since (a) It doesn't have to be positive, and (b) it has a much larger dynamic range.\n",
    "The above formulation is proposed in appendix C of the [VAE paper](https://arxiv.org/pdf/1312.6114.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `encode()` method in the `VAE` class within the `hw3/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "vae = autoencoder.VAE(encoder_cnn, decoder_cnn, x0[0].size(), z_dim).to(device)\n",
    "print(vae)\n",
    "\n",
    "z, mu, log_sigma2 = vae.encode(x0)\n",
    "\n",
    "test.assertSequenceEqual(z.shape, (1, z_dim))\n",
    "test.assertTrue(z.shape == mu.shape == log_sigma2.shape)\n",
    "\n",
    "print(f'mu(x0)={list(*mu.detach().cpu().numpy())}, sigma2(x0)={list(*torch.exp(log_sigma2).detach().cpu().numpy())}')\n",
    "\n",
    "# Sample from q(Z|x)\n",
    "N = 500\n",
    "Z = torch.zeros(N, z_dim)\n",
    "_, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    for i in range(500):\n",
    "        Z[i], _, _ = vae.encode(x0)\n",
    "        ax.scatter(*Z[i].cpu().numpy())\n",
    "\n",
    "# Should be close to the above\n",
    "print('sampled mu', torch.mean(Z, dim=0))\n",
    "print('sampled sigma2', torch.var(Z, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the full VAE Decoder, $\\Psi _{\\bb{\\beta}}(\\bb{z})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\tilde{\\vec{h}}$ from the latent vector $\\vec{z}$ using an affine transform.\n",
    "2. Reconstruct an image $\\tilde{\\vec{x}}$ from $\\tilde{\\vec{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `decode()` method in the `VAE` class within the `hw3/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`. You may need to also re-run the block above after you implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0r = vae.decode(z)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's `forward()` function will simply return `decode(encode(x))` as well as the calculated mean and log-variance of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0r, mu, log_sigma2 = vae(x0)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)\n",
    "test.assertSequenceEqual(mu.shape, (1, z_dim))\n",
    "test.assertSequenceEqual(log_sigma2.shape, (1, z_dim))\n",
    "T.functional.to_pil_image(x0r[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Implementation\n",
    "<a id=part2_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, since we're using SGD, we'll drop the expectation over $\\bb{X}$ and instead sample an instance from the training set and compute a point-wise loss. Similarly, we'll drop the expectation over $\\bb{Z}$ by sampling from $q_{\\vec{\\alpha}}(\\bb{Z}|\\bb{x})$.\n",
    "Additionally, because the KL divergence is between two Gaussian distributions, there is a closed-form expression for it. These points bring us to the following point-wise loss:\n",
    "\n",
    "$$\n",
    "\\ell(\\vec{\\alpha},\\vec{\\beta};\\bb{x}) =\n",
    "\\frac{1}{\\sigma^2} \\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  +\n",
    "\\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2 +\n",
    "\\mathrm{tr}\\,\\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) +  \\|\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})\\|^2 _2 - d_z - \\log\\det \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x})\n",
    "$$\n",
    "\n",
    "where $d_z$ is the dimension of the latent space. This pointwise loss is the quantity that we'll compute and minimize with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `vae_loss()` function in the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3.autoencoder import vae_loss\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def test_vae_loss():\n",
    "    # Test data\n",
    "    N, C, H, W = 10, 3, 64, 64 \n",
    "    z_dim = 32\n",
    "    x  = torch.randn(N, C, H, W)*2 - 1\n",
    "    xr = torch.randn(N, C, H, W)*2 - 1\n",
    "    z_mu = torch.randn(N, z_dim)\n",
    "    z_log_sigma2 = torch.randn(N, z_dim)\n",
    "    x_sigma2 = 0.9\n",
    "    \n",
    "    loss, _, _ = vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "    \n",
    "    test.assertAlmostEqual(loss.item(), 10.5053434, delta=1e-5)\n",
    "    return loss\n",
    "\n",
    "test_vae_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "<a id=part2_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of a VAE is that it can by used as a generative model by sampling the latent space, since\n",
    "we optimize for a Normal prior $p(\\bb{Z})$ in the loss function. Let's now implement this so that we can visualize how our model is doing when we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `sample()` method in the `VAE` class within the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = vae.sample(5)\n",
    "_ = plot.tensors_as_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id=part2_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Implement the `VAETrainer` class in the `hw3/training.py` module.\n",
    "2. Tweak the hyperparameters in the `part2_vae_hyperparam()` function within the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from hw3.training import VAETrainer\n",
    "from hw3.answers import part2_vae_hyperparams\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparams\n",
    "hp = part2_vae_hyperparams()\n",
    "batch_size = hp['batch_size']\n",
    "h_dim = hp['h_dim']\n",
    "z_dim = hp['z_dim']\n",
    "x_sigma2 = hp['x_sigma2']\n",
    "learn_rate = hp['learn_rate']\n",
    "betas = hp['betas']\n",
    "\n",
    "# Data\n",
    "split_lengths = [int(len(ds_gwb)*0.9), int(len(ds_gwb)*0.1)]\n",
    "ds_train, ds_test = random_split(ds_gwb, split_lengths)\n",
    "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_test  = DataLoader(ds_test,  batch_size, shuffle=True)\n",
    "im_size = ds_train[0][0].shape\n",
    "\n",
    "# Model\n",
    "encoder = autoencoder.EncoderCNN(in_channels=im_size[0], out_channels=h_dim)\n",
    "decoder = autoencoder.DecoderCNN(in_channels=h_dim, out_channels=im_size[0])\n",
    "vae = autoencoder.VAE(encoder, decoder, im_size, z_dim)\n",
    "vae_dp = DataParallel(vae).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learn_rate, betas=betas)\n",
    "\n",
    "# Loss\n",
    "def loss_fn(x, xr, z_mu, z_log_sigma2):\n",
    "    return autoencoder.vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "\n",
    "# Trainer\n",
    "trainer = VAETrainer(vae_dp, loss_fn, optimizer, device)\n",
    "checkpoint_file = 'checkpoints/vae'\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    os.remove(f'{checkpoint_file}.pt')\n",
    "\n",
    "# Show model and hypers\n",
    "print(vae)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def post_epoch_fn(epoch, train_result, test_result, verbose):\n",
    "    # Plot some samples if this is a verbose epoch\n",
    "    if verbose:\n",
    "        samples = vae.sample(n=5)\n",
    "        fig, _ = plot.tensors_as_images(samples, figsize=(6,2))\n",
    "        IPython.display.display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "else:\n",
    "    res = trainer.fit(dl_train, dl_test,\n",
    "                      num_epochs=200, early_stopping=20, print_every=10,\n",
    "                      checkpoints=checkpoint_file,\n",
    "                      post_epoch_fn=post_epoch_fn)\n",
    "    \n",
    "# Plot images from best model\n",
    "saved_state = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "vae_dp.load_state_dict(saved_state['model_state'])\n",
    "print('*** Images Generated from best model:')\n",
    "fig, _ = plot.tensors_as_images(vae_dp.module.sample(n=15), nrows=3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs236605.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What does the $\\sigma^2$ hyperparameter (`x_sigma2` in the code) do? Explain the effect of low and high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw3.answers.part2_q1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
