{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multiclass linear classification\n",
    "<a id=part3></a>\n",
    "$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial {#1}}{\\partial {#2}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll learn about loss functions and how to optimize them with gradient descent.\n",
    "We'll then use this knowledge to train a very simple model: a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "torch.random.manual_seed(1904)\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classification\n",
    "\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class linear classification we have $C$ classes which we assume our samples\n",
    "may belong to.\n",
    "We apply a linear function to a sample $x \\in \\set{R}^{D}$ and obtain a score $s_j$ which\n",
    "represents how well $x$ fits the class $1\\leq j\\leq C$ according to our model:\n",
    "$$\n",
    "s_j = \\vectr{w_j} x + b_j.\n",
    "$$\n",
    "\n",
    "Note that we have a different set of model parameters (weights) $\\vec{w_j},~b_j$ for each class,\n",
    "so a total of $C\\cdot(D+1)$ parameters.\n",
    "\n",
    "To classify a sample, we simply calculate the score for each class and choose the class with the\n",
    "highest score as our prediction.\n",
    "\n",
    "One interpretation of the weights $\\vec{w_j},~b_j$ is that they represent the parameters of an\n",
    "$N$-dimensional hyperplane. Under this interpretation the class score $s_j$ of a sample is proportional\n",
    "to the distance of that sample from the hyperplane representing the $j$-th class. Note that this score\n",
    "can be positive or negative (depending on which side of the hyperplane the sample is).\n",
    "Such a classifier therefore splits the sample space into regions where the farther a sample is from the\n",
    "positive side of a hyperplane for class $j$, the higher $s_j$, so the more likely it belongs to class $j$.\n",
    "\n",
    "![img](https://dev.datasift.com/content/1-blog/building-better-machine-learned-classifiers-faster-active-learning/700px-Hyperplane.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "In the context of supervised learning of a linear classifier model, we map a dataset\n",
    "(or batch from a dataset) of $N$ samples (for example, images flattened to vectors of length $D$)\n",
    "to a score for one of each of $C$ possible classes using the linear function above.\n",
    "\n",
    "To make the implementation efficient, we'll represent the mapping with a single matrix multiplication, employing the\n",
    "\"Bias trick\": \n",
    "Instead of both $\\vec{w_j}$ and $b_j$ per class, we'll put the bias term at the end of the weight vector and\n",
    "add a term $1$ at the end of each sample.\n",
    "\n",
    "The class scores for each sample are then given by:\n",
    "\n",
    "$$\n",
    "\\mat{S} = \\mat{X} \\mat{W}\n",
    "$$\n",
    "\n",
    "Where here (and in the code examples you'll work with),\n",
    "- $\\mat{X}$ is a matrix of shape $N\\times (D+1)$ containing $N$ samples in it's rows;\n",
    "- $\\mat{W}$ is of shape $(D+1)\\times C$ and contains the learnable classifier parameters (weights and bias);\n",
    "- $\\mat{S}$ is therefore a $N\\times C$ matrix of the class scores of each sample.\n",
    "\n",
    "Notes: \n",
    "1. In the following discussions we'll use the notation $\\vec{x_i}$ to denote the $i$-th training sample\n",
    "   (row $i$ in $\\mat{X}$) and $\\vec{w_j}$ to denote the weights and bias for class $j$ (column $j$ in $\\mat{W}$).\n",
    "   However, when writing explicit vectors we treat them all as columns, so e.g. $\\vectr{w_j}\\vec{x_i}$ is an\n",
    "   inner product.\n",
    "2. The reason we put the samples in the rows of $\\mat{X}$ and not columns (as is the convention in some texts) is\n",
    "   because that's the convention in the pytorch library: the batch dimension is always the first one. This has many\n",
    "   implementation advantages.\n",
    "\n",
    "**TODO** Implement a transform that performs the \"bias trick\" on a tensor in the module `hw1/transforms.py`.\n",
    "The following code will use your transform to load a subset of the [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "dataset for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Linear Classifier\n",
    "import torchvision.transforms as tvtf\n",
    "import hw1.datasets as hw1datasets\n",
    "import hw1.dataloaders as hw1dataloaders\n",
    "import hw1.transforms as hw1tf\n",
    "\n",
    "# Define the transforms that should be applied to each image in the dataset before returning it\n",
    "tf_ds = tvtf.Compose([\n",
    "    tvtf.ToTensor(), # Convert PIL image to pytorch Tensor\n",
    "    tvtf.Normalize(\n",
    "        # Normalize each chanel with precomputed mean and std of the train set\n",
    "        mean=(0.49139968, 0.48215841, 0.44653091),\n",
    "        std=(0.24703223,  0.24348513, 0.26158784)),\n",
    "    hw1tf.TensorView(-1), # Reshape to 1D Tensor\n",
    "    hw1tf.BiasTrick(), # Apply the bias trick (add bias dimension to data)\n",
    "])\n",
    "\n",
    "# Define how much data to load\n",
    "num_train = 10000\n",
    "num_test = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "# Training dataset\n",
    "ds_train = hw1datasets.SubsetDataset(\n",
    "    torchvision.datasets.MNIST(root='./data/mnist/', download=True, train=True, transform=tf_ds),\n",
    "    num_train)\n",
    "\n",
    "# Create training & validation sets\n",
    "dl_train, dl_valid = hw1dataloaders.create_train_validation_loaders(\n",
    "    ds_train, validation_ratio=0.2, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Test dataset & loader\n",
    "ds_test = hw1datasets.SubsetDataset(\n",
    "    torchvision.datasets.MNIST(root='./data/mnist/', download=True, train=False, transform=tf_ds),\n",
    "    num_test)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size)\n",
    "\n",
    "x0, y0 = ds_train[0]\n",
    "n_features = torch.numel(x0)\n",
    "n_classes = 10\n",
    "\n",
    "# Make sure samples have bias term added\n",
    "test.assertEqual(n_features, 28*28*1+1, \"Incorrect sample dimension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Complete the implementation of the `__init()__`, `predict()` and `evaluate_accuracy()` functions in the\n",
    "`LinearClassifier` class located in the `hw1/linear_classifier.py` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw1.linear_classifier as hw1linear\n",
    "\n",
    "# Create a classifier\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "mean_acc = 0\n",
    "for (x,y) in dl_test:\n",
    "    y_pred, _ = lin_cls.predict(x)\n",
    "    mean_acc += lin_cls.evaluate_accuracy(y, y_pred)\n",
    "mean_acc /= len(dl_test)\n",
    "\n",
    "print(f\"Accuracy: {mean_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of around 10%, corresponding to a random guess of one of ten classes. You can run the above code block multiple times to sample different initial weights and get slightly different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a linear model computes the class scores for each sample using a linear mapping as\n",
    "a score function.\n",
    "However in order to train the model, we need to define  some measure of how\n",
    "well we've classified our samples compared to their ground truth labels.\n",
    "This measure is known as a **loss function**, and it's selection is crucial in determining the model\n",
    "that will result from training. A loss function produces lower values the better the classification is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM loss function\n",
    "\n",
    "A very common linear model for classification is the Support Vector Machine. An SVM attempts to find\n",
    "separating hyperplanes that have the property of creating a maximal margin to the training samples, i.e.\n",
    "hyperplanes that are as far as possible from the closest training samples.\n",
    "For example, in the following image we see a simple case with two classes of samples that have only two features.\n",
    "The data is linearly separable and it's easy to see there are infinite possible hyperplanes (in this case lines)\n",
    "that separate the data perfectly. In this case The SVM model finds the optimal hyperplane, which is the one with\n",
    "the maximal margin. The data points closest to the separating hyperplane are called the Support Vectors\n",
    "(it can be shown that only they determine the hyperplane).\n",
    "We can see that the width of the margin is $\\frac{2}{\\norm{\\vec{w}}}$. In this simple case since the data is linearly\n",
    "separable, there exists a solution where no sample fall within the margin. If the data is not linearly separable, we\n",
    "need to allow samples to enter the margin (with a cost). This is known as a soft-margin SVM.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png\" width=400 alt=\"svm\"/>\n",
    "\n",
    "There are many ways to train an SVM model. Classically, the problem is stated as constrained optimization and\n",
    "solved with quadratic optimization techniques.\n",
    "In this exercise, we'll instead work directly with the uncontrained SVM loss function,\n",
    "calculate it's gradient analytically, and then minimize it with gradient descent.\n",
    "As we'll see in the rest of the course, this technique will be a\n",
    "major component when we train deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The in-sample loss function for a multiclass soft-margin SVM can be stated as follows:\n",
    "\n",
    "$$\n",
    "L(\\mat{W}) =\n",
    "\\frac{1}{N} \\sum_{i=1}^{N} L_{i}(\\mat{W})\n",
    "+\n",
    "\\frac{\\lambda}{2} \\norm{\\mat{W}}^2\n",
    "$$\n",
    "\n",
    "Where the first term is the mean pointwise data-dependent loss $L_{i}$,\n",
    "given by the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) formula,\n",
    "\n",
    "$$\n",
    "L_{i}(\\mat{W}) =  \\sum_{j \\neq y_i} \\max\\left(0, \\Delta+ \\vectr{w_j} \\vec{x_i} - \\vectr{w_{y_i}} \\vec{x_i}\\right),\n",
    "$$\n",
    "\n",
    "and the second term is a regularization loss which depends only on model parameters.\n",
    "Note that the hinge loss term sums over the *wrong* class prediction scores for each sample.\n",
    "This can be understood as attempting to make sure that the score for the correct class is higher than the other \n",
    "classes by\n",
    "at least some margin $\\Delta > 0$, otherwise a loss is incurred. Since this is a soft-margin SVM we allow samples\n",
    "to fall within the margin but it accumulates loss.\n",
    "The regularization term penalizes large weight magnitudes to prevent ambiguous solutions since if \n",
    "e.g. $\\mat{W^*}$ is a weight matrix that perfectly separates the data, so is $\\alpha\\mat{W^*}$ for\n",
    "any scalar $\\alpha \\geq 1$.\n",
    "\n",
    "Fitting an SVM model then amounts to finding the weight matrix $\\mat{W}$ which minimizes $L(\\mat{W})$.\n",
    "Note that we're writing the loss as a function of $\\mat{W}$ to\n",
    "emphasize that we wish to minimize it's value on the given data by with respect to the weights $\\mat{W}$,\n",
    "even though it obviously depends also on the specific dataset, $\\left\\{ \\vec{x_i}, y_i \\right\\}_{i=1}^{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "**TODO** Implement the SVM hinge loss function in the module `hw1/losses.py`, within the `SVMHingeLoss` class.\n",
    "Implement just the `loss()` function. For now you can ignore the part about saving tensors for the gradient calculation. Run the following to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs236605.dataloader_utils as dl_utils\n",
    "from hw1.losses import SVMHingeLoss\n",
    "\n",
    "# Create a hinge-loss function\n",
    "loss_fn = SVMHingeLoss(delta=1.)\n",
    "\n",
    "# Classify all samples in the test set (because it doesn't depend on initialization)\n",
    "x, y = dl_utils.flatten(dl_test)\n",
    "y_pred, x_scores = lin_cls.predict(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "\n",
    "# Compare to pre-computed expected value as a test\n",
    "expected_loss = 8.9579\n",
    "print(\"loss =\", loss.item())\n",
    "print('diff =', abs(loss.item()-expected_loss))\n",
    "test.assertAlmostEqual(loss.item(), expected_loss, delta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Loss Function with Gradient Descent\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll implement a simple gradient descent optimizer for the loss function we've implemented above. As you may recall from the lectures, the basic gradient-based optimization scheme is as follows:\n",
    "\n",
    "1. Start with initial model weights $\\mat{W_0}$ initialized randomly.\n",
    "1. For $k=1,2,\\dots,K$:\n",
    "    1. Select a step size $\\eta_k$.\n",
    "    1. Compute the gradient of the loss w.r.t. $\\mat{W}$ and evaluate at the current weights:\n",
    "        $\\nabla_{\\mat{W}} L(\\mat{W_{k-1}})$.\n",
    "    1. Update: \n",
    "        $$\n",
    "        \\mat{W_k} = \\mat{W_{k-1}} - \\eta_k \\nabla_{\\mat{W}} L(\\mat{W_{k-1}})\n",
    "        $$\n",
    "    1. Stop if minimum reached or validation-set loss is low enough.\n",
    "\n",
    "The crucial component here is the gradient calculation. In this exercise we'll analytically derive the gradient\n",
    "of the loss and then implement it in code.\n",
    "\n",
    "An important detail to note is that while $L(\\mat{W})$ is scalar-valued, it's a function of all the elements of the\n",
    "matrix $\\mat{W}$. Therefore it's gradient w.r.t. $\\mat{W}$ is also a matrix of the same shape as $\\mat{W}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mat{W}} L =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial W_{1,1}} & & \\cdots & \\frac{\\partial L}{\\partial W_{1,C}} \\\\\n",
    "    \\frac{\\partial L}{\\partial W_{2,1}} & \\ddots &  \\\\\n",
    "    \\vdots & & \\ddots &  \\\\\n",
    "    \\frac{\\partial L}{\\partial W_{D,1}} & \\cdots &  & \\frac{\\partial L}{\\partial W_{D,C}} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert \\\\\n",
    "\\frac{\\partial L}{\\partial\\vec{w_1}} & \\cdots & \\frac{\\partial L}{\\partial\\vec{w_C}}\\\\\n",
    "\\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\set{R}^{(D+1)\\times C}.\n",
    "$$\n",
    "\n",
    "For our gradient descent update-step we'll need to create such a matrix of derivatives and evaluate it at the \n",
    "current value of the weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM loss gradient\n",
    "\n",
    "The first thing we need to do is formulate an expression for the gradient of the loss function defined above. Since the expression for the loss depends on the columns of $\\mat{W}$, we'll derive an expression for the gradient of $L(\\mat{W})$ w.r.t. each $\\vec{w_j}$:\n",
    "\n",
    "$$\n",
    "\\pderiv{L}{\\vec{w_j}}(\\mat{W}) = \n",
    "\\frac{1}{N} \\sum_{i=1}^{N} \\pderiv{L_{i}}{\\vec{w_j}}(\\mat{W})\n",
    "+\n",
    "\\lambda \\mat{W}.\n",
    "$$\n",
    "\n",
    "To compute the gradient of the pointwise loss, let's define the **margin-loss** of sample $i$ for class $j$\n",
    "as follows: $m_{i,j} = \\Delta + \\vectr{w_j}\\vec{x_i} - \\vectr{w_{y_i}}\\vec{x_i}$.\n",
    "We can then write the pointwise loss and it's gradient in terms of $m_{i,j}$. We'll separate the case of $j=y_i$\n",
    "(i.e. the gradient for the correct class):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pderiv{L_i}{\\vec{w_j}} & =\n",
    "        \\begin{cases}\n",
    "            \\vec{x_i}, & m_{i,j}>0 \\\\\n",
    "            0, & \\mathrm{else} \\\\\n",
    "        \\end{cases}\n",
    "    ,~j \\neq y_i \\\\\n",
    "    \\\\\n",
    "    \\pderiv{L_i}{\\vec{w_{y_i}}} & = -\\vec{x_i} \\sum_{j\\neq y_i} \\mathbb{1}\\left( m_{i,j} > 0 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\mathbb{1}(\\cdot)$ is an indicator function that takes the value $1$ if it's argument is a true statement, else it takes $0$.\n",
    "\n",
    "Note: the hinge-loss function is not strictly speaking differentiable due to the $\\max$ operator.\n",
    "However, in practice it's not a major concern. Given that we know what argument the $\\max$ \"cooses\",\n",
    "we can differentiate each one of them separately. This is known as a sub-gradient. In the above, when $m_{i,j} \\leq 0$ we know the gradient will simply be zero.\n",
    "\n",
    "**TODO** Based on the above, implement the gradient of the loss function in the module `hw1/losses.py`,\n",
    "within the `SVMHingeLoss` class.\n",
    "Implement the `grad()` function and complete what's missing in the `loss()` function.\n",
    "Make sure you understand the above gradient derivation before attempting to implement it.\n",
    "\n",
    "Note: you'll be implementing the first term in the above equation for $\\pderiv{L}{\\vec{w_j}}(\\mat{W})$. We'll add the regularization term later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1.losses import SVMHingeLoss\n",
    "\n",
    "# Create a hinge-loss function\n",
    "loss_fn = SVMHingeLoss(delta=1)\n",
    "\n",
    "# Compute loss and gradient\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "grad = loss_fn.grad()\n",
    "\n",
    "# Test the gradient with a pre-computed expected value\n",
    "expected_grad = torch.load('tests/assets/part3_expected_grad.pt')\n",
    "diff = torch.norm(grad - expected_grad)\n",
    "print('diff =', diff.item())\n",
    "test.assertAlmostEqual(diff, 0, delta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with SGD\n",
    "<a id=part3_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our loss function and it's gradient, we can finally train our model.\n",
    "Generally, solving a machine-learning problem requires defining the following components:\n",
    "- A model:\n",
    "  architecture (type of model) consisting of hyperparameters (e.g. number of hidden layers, number of classes, etc)\n",
    "  which are set in advance and trainable parameters which we want to fit to data.\n",
    "- A loss function (sometimes denoted as a criterion):\n",
    "  evaluates the model output on some data compared to ground truth.\n",
    "- An optimization scheme:\n",
    "  specifies how the model should be updated to improve the loss. May also have hyperparameters.\n",
    "- A dataset:\n",
    "  What to fit the model to. Usually the available data is split into training, validation and test sets.\n",
    "\n",
    "\n",
    "Implementation notes:\n",
    "- You'll find that when implementing your solutions it's wise to keep the above components separate as to be\n",
    "  able to change each one of them independently from the other.\n",
    "- In this exercise we'll have separated the loss and dataset, however for simplicity we'll implement the\n",
    "  model and optimizer together.\n",
    "- As you'll see further on, `PyTorch` provides very effective mechanisms to implement all of\n",
    "  these components in a decoupled manner.\n",
    "- Note that our loss implementation didn't include regularization. We'll add this during the training phase\n",
    "  using the `weight_decay` parameter. The reason is that we prefer that the part of the loss which only depends\n",
    "  on the model parameters be part of the optimizer, not the loss function (though both ways are possible).\n",
    "  You'll see this pattern later on when you use `PyTorch`'s optimizers in the `torch.optim` package.\n",
    "- In practice we use batches of samples from the training set when training the model, because usually the training\n",
    "  set can't fit into memory. Using gradients computed on batches of data at a time is known as mini-batch\n",
    "  stochastic gradient descent (SGD).\n",
    "\n",
    "**TODO** Implement the model training loop in the `LinearClassifier`'s `train()` function.\n",
    "Use mini-batch SGD for the weight update rule.\n",
    "\n",
    "Note: You should play with the hyperparameters to get a feel for what they do to the loss and accuracy graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "\n",
    "# Evaluate on the test set\n",
    "x_test, y_test = dl_utils.flatten(dl_test)\n",
    "y_test_pred , _= lin_cls.predict(x_test)\n",
    "test_acc_before = lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
    "\n",
    "# Train the model\n",
    "svm_loss_fn = SVMHingeLoss()\n",
    "train_res, valid_res = lin_cls.train(dl_train, dl_valid, svm_loss_fn,\n",
    "                                    learn_rate=1e-3, weight_decay=0.5,\n",
    "                                    max_epochs=31)\n",
    "\n",
    "# Re-evaluate on the test set\n",
    "y_test_pred , _= lin_cls.predict(x_test)\n",
    "test_acc_after = lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
    "\n",
    "# Plot loss and accuracy\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "for i, loss_acc in enumerate(('loss', 'accuracy')):\n",
    "    axes[i].plot(getattr(train_res, loss_acc))\n",
    "    axes[i].plot(getattr(valid_res, loss_acc))\n",
    "    axes[i].set_title(loss_acc.capitalize(), fontweight='bold')\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].legend(('train', 'valid'))\n",
    "    axes[i].grid(which='both', axis='y')\n",
    "    \n",
    "# Check test set accuracy\n",
    "print(f'Test-set accuracy before training: {test_acc_before:.1f}%')\n",
    "print(f'Test-set accuracy after training: {test_acc_after:.1f}%')\n",
    "test.assertGreaterEqual(test_acc_after, 80.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a very naïve model, you should get at least 80% test set accuracy if you implemented training correctly. You can try to change the hyperparameters and see whether you get better results. Generally this should be done with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to unnderstand what models learn is to try to visualize their learned parameters.\n",
    "There can be many ways to do this. Let's try a very simple one, which is to reshape them into images of the input\n",
    "size and see what they look like.\n",
    "\n",
    "**TODO** Implement the `weights_as_images()` function in the `LinearClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs236605.plot as plot\n",
    "\n",
    "w_images = lin_cls.weights_as_images(img_shape=(1,28,28))\n",
    "fig, axes = plot.tensors_as_images(list(w_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can better understand the model by plotting some samples and looking at wrong predictions.\n",
    "Run the following block to visualize some test-set examples and the model's predictions for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images from the test set and their predictions\n",
    "n_plot = 104\n",
    "x_test, y_test = next(iter(dl_test))\n",
    "x_test = x_test[0:n_plot]\n",
    "y_test = y_test[0:n_plot]\n",
    "y_test_pred, _ = lin_cls.predict(x_test)\n",
    "x_test_img = torch.reshape(x_test[:, :-1], (n_plot, 1, 28, 28))\n",
    "\n",
    "fig, axes = plot.tensors_as_images(list(x_test_img), titles=y_test_pred.numpy(),\n",
    "                                   nrows=8, hspace=0.5, figsize=(10,8), cmap='gray')\n",
    "\n",
    "# Highlight the wrong predictions\n",
    "wrong_pred = y_test_pred != y_test\n",
    "wrong_pred_axes = axes.ravel()[wrong_pred.numpy().astype(np.bool)]\n",
    "for ax in wrong_pred_axes:\n",
    "    ax.title.set_color('red')\n",
    "    ax.title.set_fontweight('bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "<a id=part3_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple linear model we worked with, the gradient was fairly straightforward to derive analytically\n",
    "and implement.\n",
    "However for complex models such as deep neural networks with many layers and non-linear operations between\n",
    "them this is not the case. Additionally, the gradient must be re-derived any time either the model\n",
    "architecture or the loss function changes. These things make it infeasible in practice to perform\n",
    "deep-learning research using this manual method of gradient derivation.\n",
    "Therefore, all deep-learning frameworks provide a mechanism of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), to prevent\n",
    "the user from needing to manually derive the gradients of loss functions.\n",
    "\n",
    "`PyTorch` provides this functionality in a package named `torch.autograd` which we will use further on in the\n",
    "next exercises.\n",
    "For now, here's an example showing that autograd can compute the gradient of the loss function you've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier\n",
    "lin_cls = hw1linear.LinearClassifier(n_features, n_classes)\n",
    "x, y = dl_utils.flatten(dl_test)\n",
    "\n",
    "# Specify we want the gradient to be saved for the weights tensor\n",
    "lin_cls.weights.requires_grad = True\n",
    "\n",
    "# Forward pass using the weights tensor, operations will be tracked\n",
    "y_pred, x_scores = lin_cls.predict(x)\n",
    "\n",
    "# Compute loss and analytic gradient\n",
    "loss_fn = SVMHingeLoss(delta=1)\n",
    "loss = loss_fn(x, y, x_scores, y_pred)\n",
    "grad = loss_fn.grad()\n",
    "\n",
    "# Compute gradient with autograd\n",
    "loss.backward()\n",
    "autograd = lin_cls.weights.grad\n",
    "\n",
    "diff = torch.norm(grad - autograd).item()\n",
    "print('loss =', loss.item())\n",
    "print('grad =\\n', grad)\n",
    "print('autograd =\\n', autograd)\n",
    "print('diff =', diff)\n",
    "test.assertLess(diff, 1e-3, \"Gradient diff was too large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw1/answers.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs236605.answers import display_answer\n",
    "import hw1.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 \n",
    "\n",
    "Explain why the selection of $\\Delta > 0$ is arbitrary for the SVM loss $L(\\mat{W})$ as it is defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Given the images in the visualization section above,\n",
    "\n",
    "1. How do you interpret what the linear model is actually learning? Can you explain some of the classification\n",
    "   errors based on it?\n",
    "1. How is this interpretation similar or different from KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "1. Based on the graph of the training set loss, would you say that the learning rate is:\n",
    "    - Too low\n",
    "    - Good\n",
    "    - Too High\n",
    "    \n",
    "  Explain your answer by describing what the loss graph would look like in the other two cases when training\n",
    "  for the same number of epochs.\n",
    "  \n",
    "1. Based on the graph of the training and test set accuracy, would you say that the model is:\n",
    "    - Slightly overfitted to the training set\n",
    "    - Highly overfitted to the training set\n",
    "    - Slightly underfitted to the training set\n",
    "    - Highly underfitted to the training set\n",
    "    \n",
    "  and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part3_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
